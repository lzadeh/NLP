{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Beautiful views of London and information-packed Royal Observatory exhibits. \n",
      "During the week it was not crowded. I would highly recommend the Royal Observatory Greenwich.\n",
      "Find more below:\n",
      "London eyes,\n",
      "Royal Greenwich Museum and \n",
      "follow me for more info.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "html_doc = \"\"\"<html><head><title>Trip Advisor page</title></head>\n",
    "<body>\n",
    "<p> ... </p>\n",
    "<p class=\"comment-title\"><b>A must-see for a visitor to London</b></p>\n",
    "\n",
    "<p class=\"comment-body\">Beautiful views of London and information-packed Royal Observatory exhibits. \n",
    "During the week it was not crowded. I would highly recommend the Royal Observatory Greenwich.\n",
    "Find more below:\n",
    "<a href=\"http://example.com/London-eyes\" class=\"attraction\" id=\"link1\">London eyes</a>,\n",
    "<a href=\"http://example.com/sample\" class=\"attraction\" id=\"link2\">Royal Greenwich Museum</a> and \n",
    "follow me for more info.</p>\n",
    "\n",
    "<p class=\"next-comment\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "soupified = BeautifulSoup(html_doc, \"html.parser\") \n",
    "\n",
    "# extract the text of p tag with 'title' class \n",
    "title = soupified.find('p', {'class': 'comment-body'}).get_text()\n",
    "print(\"title: \" , title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Guide to the UK General Data Protection Regulation (UK GDPR)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html_doc = \"\"\"<article class=\"container\">\n",
    "    <header class=\"pageheader\">\n",
    "        <div class=\"downloadandShare\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"column column-8\">\n",
    "    <nav aria-label=\"breadcrumb\" class=\"pageheader-breadcrumb text-small clearfix\">\n",
    "        <ol>\n",
    "                <li>\n",
    "                        <a href=\"/for-organisations/\">For organisations</a><span>/</span>\n",
    "                </li>\n",
    "                <li>\n",
    "                        <a href=\"/for-organisations/guide-to-data-protection/\">Guide to Data Protection</a><span>/</span>\n",
    "                </li>\n",
    "                <li>\n",
    "                        <span class=\"current\" aria-current=\"page\" aria-label=\"Current page\">\n",
    "                            Guide to the General Data Protection Regulation (GDPR)\n",
    "                        </span>\n",
    "                </li>\n",
    "        </ol>\n",
    "    </nav>\n",
    "                    <h1 id=\"multipage-heading\">Guide to the UK General Data Protection Regulation (UK GDPR)</h1>\n",
    "                    <div id=\"multipage-snippet\">\n",
    "                        \n",
    "\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div class=\"pageheader-download column column-4 column-indent-1\">\n",
    "                    <a href=\"#\" id=\"toggle-hiddenpanel-headershare\"><span class=\"h4\">Share<span class=\"invisible\">(Opens Share panel)</span></span><span class=\"button-circle\"><span class=\"icon-share\"></span></span></a>\n",
    "                            <a href=\"#\" id=\"toggle-hiddenpanel-download\"><span class=\"h4\">Download options<span class=\"invisible\">(Opens download panel)</span></span><span class=\"button-circle\"><span class=\"icon-download\"></span></span></a>\n",
    "\n",
    "                </div>\n",
    "            </div>\n",
    "\"\"\"\n",
    "\n",
    "soupified = BeautifulSoup(html_doc, \"html.parser\") \n",
    "\n",
    "# extract the text of h1 tag with 'multipage-heading' id \n",
    "h1_text = soupified.find('h1', {'id': 'multipage-heading'}).get_text()\n",
    "print(\"Title: \" , h1_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "Alec Radford* 1Jong Wook Kim* 1Chris Hallacy1Aditya Ramesh1Gabriel Goh1Sandhini Agarwal1\n",
      "Girish Sastry1Amanda Askell1Pamela Mishkin1Jack Clark1Gretchen Krueger1Ilya Sutskever1\n",
      "Abstract\n",
      "State-of-the-art computer vision systems are\n",
      "trained to predict a Ô¨Åxed set of predetermined\n",
      "object categories. This restricted form of super-\n",
      "vision limits their generality and usability since\n",
      "additional labeled data is needed to specify any\n",
      "other visual concept. Learning directly from raw\n",
      "text about images is a promising alternative which\n",
      "leverages a much broader source of supervision.\n",
      "We demonstrate that the simple pre-training task\n",
      "of predicting which caption goes with which im-\n",
      "age is an efÔ¨Åcient and scalable way to learn SOTA\n",
      "image representations from scratch on a dataset\n",
      "of 400 million (image, text) pairs collected from\n",
      "the internet. After pre-training, natural language\n",
      "is used to reference learned visual concepts (or\n",
      "describe new ones) enabling zero-shot transfer\n",
      "of the model to downstream tasks. We study\n",
      "the performance of this approach by benchmark-\n",
      "ing on over 30 different existing computer vi-\n",
      "sion datasets, spanning tasks such as OCR, ac-\n",
      "tion recognition in videos, geo-localization, and\n",
      "many types of Ô¨Åne-grained object classiÔ¨Åcation.\n",
      "The model transfers non-trivially to most tasks\n",
      "and is often competitive with a fully supervised\n",
      "baseline without the need for any dataset spe-\n",
      "ciÔ¨Åc training. For instance, we match the ac-\n",
      "curacy of the original ResNet-50 on ImageNet\n",
      "zero-shot without needing to use any of the 1.28\n",
      "million training examples it was trained on. We\n",
      "release our code and pre-trained model weights at\n",
      "https://github.com/OpenAI/CLIP .\n",
      "1. Introduction and Motivating Work\n",
      "Pre-training methods which learn directly from raw text\n",
      "have revolutionized NLP over the last few years (Dai &\n",
      "Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\n",
      "ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\n",
      "*Equal contribution1OpenAI, San Francisco, CA 94110, USA.\n",
      "Correspondence to: <falec, jongwookg@openai.com >.Task-agnostic objectives such as autoregressive and masked\n",
      "language modeling have scaled across many orders of mag-\n",
      "nitude in compute, model capacity, and data, steadily im-\n",
      "proving capabilities. The development of ‚Äútext-to-text‚Äù as\n",
      "a standardized input-output interface (McCann et al., 2018;\n",
      "Radford et al., 2019; Raffel et al., 2019) has enabled task-\n",
      "agnostic architectures to zero-shot transfer to downstream\n",
      "datasets removing the need for specialized output heads or\n",
      "dataset speciÔ¨Åc customization. Flagship systems like GPT-3\n",
      "(Brown et al., 2020) are now competitive across many tasks\n",
      "with bespoke models while requiring little to no dataset\n",
      "speciÔ¨Åc training data.\n",
      "These results suggest that the aggregate supervision acces-\n",
      "sible to modern pre-training methods within web-scale col-\n",
      "lections of text surpasses that of high-quality crowd-labeled\n",
      "NLP datasets. However, in other Ô¨Åelds such as computer\n",
      "vision it is still standard practice to pre-train models on\n",
      "crowd-labeled datasets such as ImageNet (Deng et al., 2009).\n",
      "Could scalable pre-training methods which learn directly\n",
      "from web text result in a similar breakthrough in computer\n",
      "vision? Prior work is encouraging.\n",
      "Over 20 years ago Mori et al. (1999) explored improving\n",
      "content based image retrieval by training a model to pre-\n",
      "dict the nouns and adjectives in text documents paired with\n",
      "images. Quattoni et al. (2007) demonstrated it was possi-\n",
      "ble to learn more data efÔ¨Åcient image representations via\n",
      "manifold learning in the weight space of classiÔ¨Åers trained\n",
      "to predict words in captions associated with images. Sri-\n",
      "vastava & Salakhutdinov (2012) explored deep represen-\n",
      "tation learning by training multimodal Deep Boltzmann\n",
      "Machines on top of low-level image and text tag features.\n",
      "Joulin et al. (2016) modernized this line of work and demon-\n",
      "strated that CNNs trained to predict words in image cap-\n",
      "tions learn useful image representations. They converted\n",
      "the title, description, and hashtag metadata of images in the\n",
      "YFCC100M dataset (Thomee et al., 2016) into a bag-of-\n",
      "words multi-label classiÔ¨Åcation task and showed that pre-\n",
      "training AlexNet (Krizhevsky et al., 2012) to predict these\n",
      "labels learned representations which preformed similarly\n",
      "to ImageNet-based pre-training on transfer tasks. Li et al.\n",
      "(2017) then extended this approach to predicting phrase n-\n",
      "grams in addition to individual words and demonstrated the\n",
      "ability of their system to zero-shot transfer to other imagearXiv:2103.00020v1  [cs.CV]  26 Feb 2021\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "filename = './data/2103.00020.pdf'\n",
    "open_filename = open(filename, 'rb')\n",
    "\n",
    "pdf_reader = PyPDF2.PdfReader(open_filename)\n",
    "\n",
    "#pdf_reader = PyPDF2.PdfFileReader(BytesIO(pdf_file))\n",
    "page_obj = pdf_reader.pages[0]\n",
    "page_1 = page_obj.extract_text()\n",
    "print(page_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text extractiong from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another common source of textual data is scanned documents. Text extraction from\n",
      "scanned documents is typically done through optical character recognition (OCR),\n",
      "using libraries such as Tesseract [25, 26]. Consider the example image‚Äîa snippet\n",
      "from a 1950 article in a journal [27]‚Äîshown in Figure 2-5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract \n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\SES100\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "filename = \"./data/scanned_text.png\"\n",
    "text = pytesseract.image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\x98\\x8e HUGE LIST OF UNICODE AND EMOJI SYMBOLS TO COPY AND PASTE \\xf0\\x9f\\x98\\x80'\n"
     ]
    }
   ],
   "source": [
    "text = \"üòé HUGE LIST OF UNICODE AND EMOJI SYMBOLS TO COPY AND PASTE üòÄ\"\n",
    "unicode_text = text.encode('utf-8')\n",
    "print(unicode_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  3\n",
      "['I love the NLP module.', 'Dr. Roni and Prof. Luke are working in the AI field.', 'We expect to see 3 sentences inside this text.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "mytext = '''I love the NLP module. \n",
    "            Dr. Roni and Prof. Luke are working in the AI field.\n",
    "            We expect to see 3 sentences inside this text.'''\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "print(\"Number of sentences: \" , len(my_sentences))\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the NLP module.\n",
      "\n",
      "            Dr. Roni and Prof. Luke are working in the AI field.\n",
      "\n",
      "            \n",
      "We expect to see 3 sentences inside this text.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "textObj = nlp(mytext)\n",
    "for sentence in textObj.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  I love the NLP module.\n",
      "Tokens:  ['I', 'love', 'the', 'NLP', 'module', '.']\n",
      "Sentence:  Dr. Roni and Prof. Luke are working in the AI field.\n",
      "Tokens:  ['Dr.', 'Roni', 'and', 'Prof.', 'Luke', 'are', 'working', 'in', 'the', 'AI', 'field', '.']\n",
      "Sentence:  We expect to see 3 sentences inside this text.\n",
      "Tokens:  ['We', 'expect', 'to', 'see', '3', 'sentences', 'inside', 'this', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    my_words = word_tokenize(sentence)\n",
    "    print(\"Sentence: \", sentence)\n",
    "    print(\"Tokens: \", my_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['There', 'are', '$', '10,000', 'and', '‚Ç¨1000', 'which', 'are', 'there', 'just', 'for', 'testing', 'a', 'tokenizer']\n"
     ]
    }
   ],
   "source": [
    "sentence = '''There are $10,000 and ‚Ç¨1000 which are there \n",
    "        just for testing a tokenizer'''\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['Let', \"'s\", 'go', 'to', 'N.Y.', '!']\n"
     ]
    }
   ],
   "source": [
    "sentence = '''Let's go to N.Y.!'''\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SES100\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['There', '$', '10,000', '‚Ç¨1000', 'testing', 'tokenizer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [ token for token in word_tokenize(sentence) \n",
    "                if token not in stop_words ]\n",
    "    print(\"Tokens: \", tokens)\n",
    "\n",
    "text = \"There are $10,000 and ‚Ç¨1000 which are there just for testing a tokenizer\"\n",
    "remove_stop_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'through', 'needn', \"hadn't\", 'him', 'as', 'doesn', 'are', 'further', 'is', 'such', 'weren', 'few', \"that'll\", 'both', 're', 'and', 'while', 'has', 'its', 'me', 'with', \"shouldn't\", 'theirs', 'too', 'when', 'shouldn', 'ourselves', 'hadn', 'below', 'shan', 'no', 'a', \"shan't\", 'same', 'yourselves', 'themselves', 'herself', 'who', 'to', 'ain', 'wasn', \"didn't\", 'but', 'between', 'that', 'have', 'ma', 'don', 'about', 'll', 'against', 't', 'after', 'off', 'you', 'ours', 'under', \"she's\", 'whom', 'during', 'what', 'he', 'before', 'am', \"mightn't\", 'how', \"couldn't\", 'of', 'itself', 'here', 'if', 'this', \"you'd\", 'from', 'wouldn', 'or', 'be', \"you're\", 'all', 'these', 'been', 'nor', 'above', 'very', 's', 'himself', 'why', 'should', 'only', 'mightn', 'out', 'i', 'then', \"you've\", \"haven't\", 'at', 'down', 'isn', 'will', 'o', \"hasn't\", 'each', \"wasn't\", 'until', 'won', 'up', 'having', \"weren't\", 'myself', 'hers', 'did', 'can', \"won't\", 'an', 'haven', 'being', 'which', \"mustn't\", 'my', 'now', 'those', 'we', 'm', \"aren't\", 'couldn', 'most', 've', 'mustn', 'on', 'it', 'your', \"isn't\", 'his', 'over', 'more', 'for', 'didn', 'there', 'once', 'do', 'd', 'yours', 'into', 'aren', \"don't\", 'her', 'some', 'them', 'in', 'just', \"wouldn't\", 'not', \"you'll\", \"should've\", \"needn't\", 'own', 'so', 'they', 'she', 'does', 'doing', 'where', 'their', 'were', \"doesn't\", 'than', 'our', 'hasn', 'the', 'yourself', 'again', 'was', 'other', \"it's\", 'because', 'any', 'by', 'y', 'had'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'code', 'show', 'you', 'how', 'stem', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"This code shows you how stemming works.\"\n",
    "words = [ stemmer.stem(t) for t in word_tokenize(text) ]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', \"'s\", 'good', 'to', 'learn', 'nlp', '.']\n",
      "['it', \"'s\", 'better', 'to', 'learn', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "text1 = \"It's good to learn NLP.\"\n",
    "words = [ stemmer.stem(t) for t in word_tokenize(text1) ]\n",
    "print(words)\n",
    "\n",
    "text2 = \"It's better to learn NLP.\"\n",
    "words = [ stemmer.stem(t) for t in word_tokenize(text2) ]\n",
    "print(words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SES100\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SES100\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'turn', 'right', 'at', 'the', 'next', 'light', '.']\n",
      "['She', 'is', 'always', 'right', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text1 = \"Please turn right at the next light.\"\n",
    "words = [ lemmatizer.lemmatize(t, pos='a') for t in word_tokenize(text1) ]\n",
    "print(words)\n",
    "\n",
    "text2 = \"She is always right.\"\n",
    "words = [ lemmatizer.lemmatize(t, pos='a') for t in word_tokenize(text2) ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'good', 'to', 'learn', 'NLP', '.']\n",
      "['It', \"'s\", 'good', 'to', 'learn', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text1 = \"It's good to learn NLP.\"\n",
    "words = [ lemmatizer.lemmatize(t, pos='a') \n",
    "            for t in word_tokenize(text1) ]\n",
    "print(words)\n",
    "\n",
    "text2 = \"It's better to learn NLP.\"\n",
    "words = [ lemmatizer.lemmatize(t, pos='a') \n",
    "            for t in word_tokenize(text2) ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'It': 'it', \"'s\": 'be', 'better': 'well', 'to': 'to', 'learn': 'learn', 'NLP': 'NLP', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text1 = \"It's better to learn NLP.\"\n",
    "words = [ sp(t) for t in word_tokenize(text1) ]\n",
    "lemmatized_dic = {}\n",
    "for word in words:\n",
    "    lemmatized_dic[word[0].text] = word[0].lemma_\n",
    "\n",
    "print(lemmatized_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit/punctuation removal & lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'test', 'to', 'remove', 'digits', 'punctuation.', 'marks']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from string import punctuation\n",
    "\n",
    "text1 = \"A test! to $100 @ remove, Digits & Punctuation.! marks?\"\n",
    "\n",
    "def remove_digits_punctuation_lower_case(text):\n",
    "    return [token.lower() for token in word_tokenize(text) \n",
    "        if not token.isnumeric() and token not in punctuation]\n",
    "\n",
    "print(remove_digits_punctuation_lower_case(text1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It it PRON Xx True True\n",
      "'s be AUX 'x False True\n",
      "better well ADJ xxxx True False\n",
      "to to PART xx True True\n",
      "learn learn VERB xxxx True False\n",
      "NLP NLP PROPN XXX True False\n",
      ". . PUNCT . False False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('It\\'s better to learn NLP.')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, \n",
    "    token.shape_, token.is_alpha, token.is_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "778111f43dd4467f233eb510082683772f85150aae7e84a1db9ec8d6d394ff07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
