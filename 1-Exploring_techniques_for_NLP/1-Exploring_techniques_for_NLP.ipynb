{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring techniques for NLP\n",
    "As we mentioned in the lecture slides, the different approaches used to solve NLP problems commonly fall into three categories: \n",
    "- rule-based, \n",
    "- machine learning, and \n",
    "- deep learning. \n",
    "\n",
    "In this notebook we will try to show you how to use different approaches to solve NLP problems. \n",
    "\n",
    "You can open the cloud version of this notebook using the following link:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lzadeh/NLP/blob/main/1-Exploring_techniques_for_NLP.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based technique\n",
    "Similar to other early AI systems, early attempts at designing NLP systems were based on building rules for the task at hand. This required that the developers had some expertise in the domain to formulate rules that could be incorporated into a program. Such systems also required resources like dictionaries and thesauruses, typically compiled and digitized over a period of time.\n",
    "\n",
    "Regular expressions (regex) are a great tool for text analysis and building rule-based systems. A regex is a set of characters or a pattern that is used to match and find substrings in text. For example, a regex like <b><font color='maroon'>‘^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.] +)\\.([a-zA-Z]{2,5})$’</font></b> is used to find all email IDs in a piece of text. Regexes are a great way to incorporate domain knowledge in your NLP system. For example, given a job advertisement, we want to build a system to automatically identify the contract type, the job title, and the salary. There are a variety of contract types and salary ranges available. These can be easily matched with regexes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: Consider a recruiting agent that tries to match candidates with available opportunities by identifying contract type, job title, and salary range from the following list of job descriptions:\n",
    "\n",
    "- <b>Job Description 1:</b> \"We are looking for a full-time software engineer to join our team. The salary for this position is $100,000 per year.\"\n",
    "\n",
    "- <b>Job Description 2:</b> \"Our company is offering a part-time administrative assistant job with a salary range of $20-25 per hour.\" \n",
    "\n",
    "- <b>Job Description 3:</b> \"This is a 6-month internship opportunity for a data analyst. The contract rate is $75,000 per year.\" \n",
    "\n",
    "Regular expression can be used to find all the match for contact types, job titles and salary ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('full-time', 'Software engineer', '$100,000')\n",
      "('part-time', 'Administrative assistant', '$20-25')\n",
      "('internship', 'Data analyst', '$75,000')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_information(text):\n",
    "  contract_pattern = r'(full-time|part-time|contract|internship)'\n",
    "  title_pattern = r'^Title: ([a-zA-Z]+ [a-zA-Z]+){1}'\n",
    "  salary_pattern = r'\\$(\\d+(,|-)+\\d+)'\n",
    "  \n",
    "  contract = re.search(contract_pattern, text, re.IGNORECASE)\n",
    "  title = re.search(title_pattern, text, re.IGNORECASE)\n",
    "  salary = re.search(salary_pattern, text, re.IGNORECASE)\n",
    "  \n",
    "  if contract:\n",
    "    contract = contract.group(1)\n",
    "  if title:\n",
    "    title = title.group(1)\n",
    "  if salary:\n",
    "    salary = salary.group(0)\n",
    "  \n",
    "  return contract, title, salary\n",
    "\n",
    "text1 = \"Title: Software engineer, Description: We are looking for a full-time software engineer to join our team. The salary for this position is $100,000 per year.\"\n",
    "text2 = \"Title: Administrative assistant, Description: Our company is offering a part-time administrative assistant job with a salary range of $20-25 per hour.\"\n",
    "text3 = \"Title: Data analyst, Description: This is a 6-month internship opportunity for a data analyst. The contract rate is $75,000 per year.\"\n",
    "\n",
    "print(extract_information(text1))\n",
    "print(extract_information(text2))\n",
    "print(extract_information(text3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expressions used in this code are designed to capture common patterns in job post data, but they may not capture all possible variations. The code can be further refined and improved to capture more specific cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information retrival\n",
    "One major challenge in NLP is creating structured data from unstructured and/or semi-structured documents. Information extraction is one of the NLP tasks to retrieve structured data from normal text. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The following texts have been selected from a newspaper about the best players of the Champion League in 2022:\n",
    "\n",
    "- Karim Mostafa Benzema is a French professional footballer who plays as a striker for La Liga club Real Madrid and the France national team.\n",
    "- Kylian Mbappé Lottin is a French professional footballer who plays as a forward for Ligue 1 club Paris Saint-Germain and the France national team \n",
    "- Lewandowski is a Polish professional footballer who plays as a striker for Bundesliga club Bayern Munich and is the captain of the Poland national team. \n",
    "\n",
    "Let's create a function and pass a pattern and text to return the first match of the pattern inside the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_text(pattern, text):\n",
    "    match_items = re.findall(pattern, text)\n",
    "    if match_items:\n",
    "        return match_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = 'Karim Mostafa Benzema is a French professional footballer who plays as a striker for La Liga club Real Madrid and the France national team.'\n",
    "player2 = 'Kylian Mbappé Lottin is a French professional footballer who plays as a forward for Ligue 1 club Paris Saint-Germain and the France national team'\n",
    "player3 = 'Lewandowski is a Polish professional footballer who plays as a striker for Bundesliga club Bayern Munich and is the captain of the Poland national team.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Exercise 1 </font>\n",
    "Write a pattern to extract name of first player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karim Mostafa Benzema '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write a pattern to extract name of the player1\n",
    "pattern = '?'\n",
    "get_match_text(pattern ,player1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Exercise 2 </font>\n",
    "Write a pattern to extract the club name for the second player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris Saint-Germain'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write a pattern to extract the club name for the second player.\n",
    "pattern = '?'\n",
    "get_match_text(pattern ,player2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Exercise 3 </font>\n",
    "Write a pattern to retrive the national team name for third player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poland '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern to retrive the national team name for third player.\n",
    "pattern = '?'\n",
    "get_match_text(pattern ,player3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for NLP\n",
    "In the following sections you can see some interesting examples of deep learning techniques for NLP.\n",
    "- Sample 1: Sentiment analysis\n",
    "- Sample 2: Text generation \n",
    "- Sample 3: Question answering\n",
    "\n",
    "Transformer is one of the advance deep learning techniques for executing various NLP tasks. We will cover them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to install tensorflow and transformers packages\n",
    "# !pip install tensorflow\n",
    "# !pip install -q transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SES100\\Anaconda3\\envs\\nlp_techniques\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import transformers pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997692704200745},\n",
       " {'label': 'NEGATIVE', 'score': 0.9990854263305664}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a ready to use sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love NLP\", \"I hate NLP\"]\n",
    "sentiment_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 331kB/s]\n",
      "c:\\Users\\SES100\\Anaconda3\\envs\\nlp_techniques\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SES100\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)\"tf_model.h5\";: 100%|██████████| 498M/498M [03:55<00:00, 2.11MB/s] \n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 61.5kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 652kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 557kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01<00:00, 812kB/s]\n",
      "c:\\Users\\SES100\\Anaconda3\\envs\\nlp_techniques\\lib\\site-packages\\transformers\\generation\\tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Natural language processing (NLP) is thought to be one of the key factors in improving verbal performance due to its influence on memory in humans. However'},\n",
       " {'generated_text': 'Natural language processing (NLP) is considered to be the cornerstone of the field of linguistics and of cognitive science. The goal is to obtain the'},\n",
       " {'generated_text': 'Natural language processing (NLP) is the core tool in NLP programs. NLP consists of two or more languages whose standard features (such as'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating text using a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model = 'gpt2') \n",
    "text_generator(\"Natural language processing (NLP) is\", max_length = 30, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_76']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9869260191917419,\n",
       " 'start': 1,\n",
       " 'end': 28,\n",
       " 'answer': 'Natural language processing'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answering questions using a question answering pipeline\n",
    "nlp = pipeline(\"question-answering\") \n",
    "context = \"\"\" Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "                and artificial intelligence concerned with the interactions between computers \n",
    "                and human language, in particular how to program computers to process and analyze \n",
    "                large amounts of natural language data. \n",
    "                The result is a computer capable of \"understanding\" the contents of documents, \n",
    "                including the contextual nuances of the language within them. The technology can \n",
    "                then accurately extract information and insights contained in the documents \n",
    "                as well as categorize and organize the documents themselves. \"\"\" \n",
    "nlp(question=\"What is NLP?\", context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb296794c73b83efe7c98d8d075eb94e061b1a5f25f316abf91fc8372fd30430"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
